\subsection{Data preprocessing}
We randomly split data into train (80) and test (20) subsets using \href{http://scikit-learn.org/stable/modules/classes.html}{\textcolor{blue}{sklearn.model\_ selection.train\_ test\_ split}}. The ground-truth labels of data will be get from the folder from original dataset and then it will be stored in \textbf{features/y\_ train.pkl} and \textbf{features/y\_ test.pkl} for using in python code.

\subsection{Feature extraction}
\paragraph*{}
The BOV vocabulary is built up by using all images in dataset. We get about 40 - 50 key points and its descriptors in every image, then run K-Means with \textbf{k = 500} on this key points space to get the vocabulary with 500 visual words. The vocabulary is stored in \textbf{bin/vocabulary.pkl }

\paragraph*{}
After having the vocabulary, we calculate the histogram of visual words for all images in both training set and test set. The result (feature vectors of training and test set) is stored in \textbf{features/X\_ train.pkl} and \textbf{features/X\_ test.pkl}

\subsection{KNN test}
We simply run KNN on test set with diffrent k (from 1 to 100) and end up with result below.

\subsection{SVMs test}
\paragraph*{}
We try to train SVM model with different parameter values, different kernels and aproachs for multi-classification. All models are saved in \textbf{bin/svm-models} with format "\textbf{multi-class\_ kernel\_ c\_ degree\_ gamma}". The value of parameter is changing following the conclusion from theory we've learn in the class and our experiences in the testing process. 
\paragraph*{}
Here is some results (for more detail, you can read the log files from \textbf{log/svm}).

\begin{table}[h!]
  \begin{center}
    \label{linear}
    \pgfplotstabletypeset[
    	multicolumn names,
		col sep=comma,
		string type,
      	display columns/0/.style={column name=$method$, 			column type={c}},
	  	display columns/1/.style={column name=$C$, 					column type={r}},
	  	display columns/2/.style={column name=$training-time$,		column type={S}},
	  	display columns/3/.style={column name=$training-accuracy$,	column type={S}},
	  	display columns/4/.style={column name=$test-accuracy$,		column type={S}},
     	every head row/.style={before row={\toprule}, after row={\midrule}},
		every last row/.style={after row=\bottomrule},
    ]{tables/svm/linear.csv}
    \caption{SVM result with linear kernel.}
  \end{center}
\end{table}

\begin{table}[h!]
  \begin{center}
    \label{poly}
    \pgfplotstabletypeset[
    	multicolumn names,
		col sep=comma,
		string type,
      	display columns/0/.style={column name=$method$, 			column type={c}},
	  	display columns/1/.style={column name=$C$, 					column type={r}},
	  	display columns/2/.style={column name=$degree$, 			column type={c}},
	  	display columns/3/.style={column name=$training-time$,		column type={S}},
	  	display columns/4/.style={column name=$training-accuracy$,	column type={S}},
	  	display columns/5/.style={column name=$test-accuracy$,		column type={S}},
     	every head row/.style={before row={\toprule}, after row={\midrule}},
		every last row/.style={after row=\bottomrule},
    ]{tables/svm/poly.csv}
    \caption{SVM result with poly kernel.}
  \end{center}
\end{table}

\pagebreak
\begin{table}[h!]
  \begin{center}
    \label{rbf}
    \pgfplotstabletypeset[
    	multicolumn names,
		col sep=comma,
		string type,
      	display columns/0/.style={column name=$method$, 			column type={c}},
	  	display columns/1/.style={column name=$C$, 					column type={r}},
	  	display columns/2/.style={column name=$gamma$, 				column type={r}},
	  	display columns/3/.style={column name=$training-time$,		column type={S}},
	  	display columns/4/.style={column name=$training-accuracy$,	column type={S}},
	  	display columns/5/.style={column name=$test-accuracy$,		column type={S}},
     	every head row/.style={before row={\toprule}, after row={\midrule}},
		every last row/.style={after row=\bottomrule},
    ]{tables/svm/poly.csv}
    \caption{SVM result with rbf kernel.}
  \end{center}
\end{table}

\subsubsection*{Some theories have been proven}
\begin{itemize}
\item "One Vs One" accuracy is higher than "One Vs Rest" (shown in \ref{tab:linear})
\item "One Vs Rest" accuracy is faster than "One Vs One" (shown in \ref{tab:linear})
\item If the value of parameter C for SVM soft-margin is great enough, soft margin will behave like hard margin (shown in \ref{tab:linear} the One Vs One method, C = 0.2 or greater we have the same result (and it also is the hard-margin result)).
\item The more complex the model is, the more time takes to train and the trained model can be cause of over-fitting (the data is more simpler than what the model represent). (In \ref{tab:poly} when \textit{degree} becomes greate or in \ref{tab:rbf} when \textit{gamma} becomes greater, it takes a lot of time to train and the trained model fits almost 100\% training data)
\end{itemize}